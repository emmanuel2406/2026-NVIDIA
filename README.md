# [Your Team Name] - NVIDIA iQuHACK 2026 Challenge Submission

## ðŸŽ¯ Project Overview

This repository contains our team's solution for the **NVIDIA iQuHACK 2026 Challenge**, tackling the Low Autocorrelation of Binary Sequences (LABS) optimization problem through a hybrid quantum-classical approach with GPU acceleration.


---

## ðŸ“‹ Table of Contents

1. [Our Approach](#-our-approach)
2. [Implementation Summary](#-implementation-summary)
3. [Results & Performance](#-results--performance)
4. [Repository Navigation](#-repository-navigation)
5. [Phase 1: Prototyping (qBraid)](#-phase-1-prototyping-qbraid)
6. [Phase 2: GPU Acceleration (Brev)](#-phase-2-gpu-acceleration-brev)
7. [AI-Assisted Development Workflow](#-ai-assisted-development-workflow)
8. [How to Run Our Code](#-how-to-run-our-code)
9. [Challenges & Solutions](#-challenges--solutions)
10. [Key Learnings](#-key-learnings)
11. [Acknowledgments](#-acknowledgments)

---

## ðŸš€ Our Approach

### Problem Understanding

The LABS problem seeks binary sequences that minimize autocorrelation sidelobes - critical for radar and telecommunications applications. We tackled this NP-hard optimization problem by:

**[Describe your team's high-level strategy]**

Example:
- âœ… Starting with classical Memetic Tabu Search (MTS) baseline
- âœ… Implementing QAOA-based quantum enhancement for population seeding
- âœ… GPU-accelerating both quantum simulation and classical search
- âœ… Benchmarking across multiple NVIDIA GPU architectures

### Why This Approach?

**[Explain your reasoning for choosing your specific quantum algorithm and hybrid strategy]**

Example:
- QAOA provides variational flexibility for combinatorial problems
- Quantum seeding can explore solution space more efficiently than random initialization
- GPU acceleration critical for scaling to larger sequence lengths
- Hybrid approach leverages strengths of both quantum and classical computing

---

## ðŸ’» Implementation Summary

### Quantum Algorithm: [Your Choice - QAOA/QMF/Trotter/Custom]

**[Describe your quantum implementation]**

Example structure:
```
Circuit Design:
- [X] qubits for sequence length N
- [Y] QAOA layers (p-value)
- Mixer Hamiltonian: [describe]
- Cost Hamiltonian: [describe]

Parameter Optimization:
- Optimizer: [e.g., COBYLA, Adam, etc.]
- Convergence criteria: [describe]
- Number of iterations: [X]
```

### Classical Component: MTS Enhancement

**[Describe how you integrated quantum with classical MTS]**

Example:
- Quantum samples used to initialize top 20% of MTS population
- Classical refinement through tabu search iterations
- Hybrid evaluation function combining quantum and classical metrics

### GPU Optimization Strategy

**[Describe your GPU acceleration approach]**

Example:
- Parallelized quantum circuit simulation using CUDA-Q GPU backend
- Batch processing of multiple candidate solutions
- Optimized memory transfers between CPU and GPU
- Architecture-specific tuning for [L4/T4/A100]

---

## ðŸ“Š Results & Performance

### Solution Quality

**Best LABS Solutions Found:**

| Sequence Length | Energy Merit E(s) | Method | Time (seconds) |
|-----------------|-------------------|--------|----------------|
| 20              | [value]           | [method] | [time]       |
| 40              | [value]           | [method] | [time]       |
| 60              | [value]           | [method] | [time]       |
| [Your sizes]    | [values]          | [methods] | [times]     |

**[Add comparison to known optimal solutions if available]**

### Performance Benchmarks

#### CPU vs GPU Speedup

| Task | CPU Time | GPU Time (L4) | GPU Time (A100) | Speedup |
|------|----------|---------------|-----------------|---------|
| Quantum Circuit Simulation | [X]s | [Y]s | [Z]s | [A]x |
| MTS Population Update | [X]s | [Y]s | [Z]s | [A]x |
| Full Hybrid Iteration | [X]s | [Y]s | [Z]s | [A]x |

**[Add your actual performance data]**

#### Scaling Analysis

**[Include graphs or descriptions of how your solution scales with:]**
- Sequence length
- Number of QAOA layers
- GPU architecture
- Population size

### Key Findings

**[Summarize your main discoveries]**

Example structure:
1. **Quantum Enhancement Impact:** Quantum-seeded populations converged [X]% faster than random initialization
2. **GPU Architecture:** A100 provided [Y]x speedup over L4 for [specific task]
3. **Optimal Configuration:** Best results with [p=? QAOA layers, population size=?, etc.]
4. **Bottlenecks Identified:** [What limited performance?]

---

## ðŸ“ Repository Navigation

### What We Modified/Created

**Original Challenge Structure:**
- `tutorial_notebook/` - Challenge introduction and tutorials
- `impl-mts/` - Base MTS implementation
- `impl-qaoa/`, `impl-qmf/`, `impl-trotter/` - Example quantum approaches

**Our Team's Additions:**

#### `/team-submissions/[YourTeamName]/`
Our main deliverables folder containing:
- **`final_report.pdf`** - Comprehensive project report
- **`presentation.pdf`** - Final presentation slides
- **`retrospective.md`** - Team reflection and lessons learned
- **`deliverables_checklist.md`** - Completed evaluation criteria

#### `/[your-implementation-folder]/`
**[Point to your main code directory]**

Example:
```
/our-hybrid-solution/
â”œâ”€â”€ quantum_enhanced_mts.py      # Main hybrid algorithm
â”œâ”€â”€ qaoa_labs_solver.py          # QAOA implementation
â”œâ”€â”€ gpu_kernels.py               # GPU-optimized functions
â”œâ”€â”€ benchmarking.py              # Performance evaluation
â”œâ”€â”€ utils.py                     # Helper functions
â””â”€â”€ README.md                    # Implementation details
```

#### `/results/[YourTeamName]/`
Our experimental results:
- Performance logs
- Solution quality data
- Benchmark comparisons
- Visualization plots

#### `/benchmarks/[YourTeamName]/`
Benchmarking scripts and outputs:
- Cross-architecture comparisons
- Scalability tests
- Convergence analysis

---

## ðŸ”¬ Phase 1: Prototyping (qBraid)

### What We Accomplished

**Milestone 1 - Ramp Up (Completed: [Date])**
- âœ… Completed tutorial notebook
- âœ… Understood classical MTS baseline
- âœ… Familiarized with CUDA-Q syntax and quantum gates
- âœ… [Other achievements]

**Milestone 2 - Research & Plan (Completed: [Date])**
- âœ… Literature review on [specific papers/approaches]
- âœ… Evaluated QAOA vs QMF vs Trotter for LABS
- âœ… Decided on [chosen approach] because [reasoning]
- âœ… Designed hybrid workflow architecture
- âœ… Created development timeline and task assignments
- âœ… [Other achievements]

### Phase 1 Deliverables

**CPU-Validated Implementation:**
- File: `[path/to/your/implementation.py]`
- Validation: `[path/to/test_suite.py]`
- Results: Successfully found solutions for N=[sizes] on CPU

**Technical Design Document:**
- File: `team-submissions/[YourTeamName]/technical_design.md`
- Includes: Algorithm description, architecture diagrams, GPU acceleration plan

---

## âš¡ Phase 2: GPU Acceleration (Brev)

### Migration from qBraid to Brev

**GPU Acceleration PIC:** [Name]

**Migration Steps Completed:**
1. âœ… Accessed Brev platform with team credits
2. âœ… Launched NVIDIA GPU environment
3. âœ… Transferred code from qBraid
4. âœ… Configured CUDA-Q GPU backend
5. âœ… Validated functionality on GPU

### GPU Optimization Process

**[Describe what you did to optimize for GPU]**

Example:
- Profiled code to identify bottlenecks using [tools]
- Optimized quantum circuit simulation by [specific changes]
- Parallelized classical MTS components via [approach]
- Reduced memory overhead through [techniques]
- Tuned for specific GPU architecture by [methods]

### Multi-Architecture Testing

**GPUs Tested:**
- [X] NVIDIA L4: [findings]
- [X] NVIDIA T4: [findings]
- [X] NVIDIA A100: [findings]

**Optimal Configuration:** [Which GPU worked best for your workload and why]

---

## ðŸ¤– AI-Assisted Development Workflow

### How We "Vibe Coded"

**[Describe your team's AI integration strategy]**

Example structure:

**Tools Used:**
- [X] Claude AI / ChatGPT / GitHub Copilot for [specific tasks]
- [X] CODA Platform for [quantum algorithm exploration]
- [X] [Other tools]

**Agentic Strategies Employed:**

1. **Project Lead Tasks:**
   - Used AI for: [e.g., documentation generation, project planning]
   - Prompt example: [share an effective prompt]
   - Verification approach: [how you checked AI output]

2. **GPU Acceleration Tasks:**
   - Used AI for: [e.g., kernel optimization, profiling analysis]
   - Prompt example: [share an effective prompt]
   - Verification approach: [how you checked AI output]

3. **Quality Assurance Tasks:**
   - Used AI for: [e.g., test generation, bug detection]
   - Prompt example: [share an effective prompt]
   - Verification approach: [how you checked AI output]

4. **Technical Marketing Tasks:**
   - Used AI for: [e.g., documentation, visualizations]
   - Prompt example: [share an effective prompt]
   - Verification approach: [how you checked AI output]

**What Worked Well:**
- [AI strength 1]
- [AI strength 2]

**What Required Human Oversight:**
- [Area requiring verification 1]
- [Area requiring verification 2]

**Context Management:**
- Used `skills.md` for [purpose]
- Employed [context tools/methods]
- Prompt engineering techniques: [list]

---

## ðŸ› ï¸ How to Run Our Code

### Prerequisites

```bash
# Python environment
python >= 3.9

# Install dependencies
pip install -r requirements.txt

# CUDA-Q installation (qBraid has this pre-installed)
# For local setup, follow: https://nvidia.github.io/cuda-quantum/latest/install.html
```

### Quick Start on qBraid

```bash
# 1. Open tutorial notebook to understand the problem
jupyter notebook tutorial_notebook/01_quantum_enhanced_optimization_LABS.ipynb

# 2. Run our implementation
cd [your-implementation-folder]
python quantum_enhanced_mts.py --sequence-length 40 --backend cpu

# 3. Run benchmarks
cd benchmarks/[YourTeamName]
python run_benchmarks.py
```

### Running on Brev (GPU)

```bash
# 1. Ensure you're in Brev environment with GPU access
nvidia-smi  # Check GPU availability

# 2. Run with GPU acceleration
cd [your-implementation-folder]
python quantum_enhanced_mts.py --sequence-length 60 --backend nvidia

# 3. Run GPU benchmarks
python gpu_benchmarks.py --gpu-type A100
```

### Reproducing Our Results

```bash
# Run complete experiment suite
bash scripts/reproduce_results.sh

# This will:
# - Generate LABS solutions for multiple sequence lengths
# - Benchmark CPU vs GPU performance
# - Create visualization plots
# - Output results to results/[YourTeamName]/
```

---

## ðŸ”¥ Challenges & Solutions

### Technical Challenges

**Challenge 1: [Describe a major technical challenge]**
- **Problem:** [What went wrong]
- **Solution:** [How you solved it]
- **Lesson:** [What you learned]

**Challenge 2: [Describe another challenge]**
- **Problem:** [What went wrong]
- **Solution:** [How you solved it]
- **Lesson:** [What you learned]

**Challenge 3: [Platform/Environment Issues]**
- **Problem:** [e.g., qBraid to Brev migration issues]
- **Solution:** [How you overcame it]
- **Lesson:** [What you learned]

### Team Coordination Challenges

**[Describe any workflow or coordination challenges]**
- How you divided work
- How you maintained alignment
- How you integrated different components

---

## ðŸ“š Key Learnings

### Technical Insights

**About Quantum Computing:**
- [Insight 1]
- [Insight 2]
- [Insight 3]

**About GPU Programming:**
- [Insight 1]
- [Insight 2]
- [Insight 3]

**About Hybrid Algorithms:**
- [Insight 1]
- [Insight 2]
- [Insight 3]

### Process Insights

**About AI-Assisted Development:**
- [What worked with AI agents]
- [What required human expertise]
- [Best practices discovered]

**About Hackathon Development:**
- [Time management lessons]
- [Rapid prototyping techniques]
- [Verification strategies]

### What We'd Do Differently

**[Reflect on improvements for future work]**
- If we had more time: [...]
- Alternative approaches to try: [...]
- Better optimization strategies: [...]

---

## ðŸŽ‰ Acknowledgments

### Special Thanks

- **NVIDIA** for designing this incredible challenge and providing GPU resources
- **MIT iQuISE** for organizing iQuHACK 2026
- **qBraid** for the seamless development platform
- **Brev** for GPU infrastructure access
- **Conductor Quantum** for CODA platform credits
- **Challenge Mentors:** [Names if you worked with specific mentors]
- **Our AI Assistants:** For being tireless coding partners ðŸ¤–

### Resources That Helped Us

**Most Valuable Resources:**
- [Specific tutorial/paper/documentation that was crucial]
- [Another key resource]
- [Community help or mentor advice]

---


